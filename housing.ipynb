{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'max_features': 8, 'n_estimators': 30}\n",
      "63467.324961700964 {'max_features': 2, 'n_estimators': 3}\n",
      "54539.981728016224 {'max_features': 2, 'n_estimators': 10}\n",
      "52683.33906858547 {'max_features': 2, 'n_estimators': 30}\n",
      "59358.19799425676 {'max_features': 4, 'n_estimators': 3}\n",
      "52271.45838211252 {'max_features': 4, 'n_estimators': 10}\n",
      "50391.70821355501 {'max_features': 4, 'n_estimators': 30}\n",
      "58524.37129023218 {'max_features': 6, 'n_estimators': 3}\n",
      "51889.73782864724 {'max_features': 6, 'n_estimators': 10}\n",
      "49787.88754867301 {'max_features': 6, 'n_estimators': 30}\n",
      "58266.43007066911 {'max_features': 8, 'n_estimators': 3}\n",
      "51903.51994488712 {'max_features': 8, 'n_estimators': 10}\n",
      "49766.992373872185 {'max_features': 8, 'n_estimators': 30}\n",
      "62635.17490518263 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54368.75469014809 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "60175.36801146478 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52422.9453105846 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "57759.30974710149 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51525.88868173429 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                        bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "    \n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n",
    "\n",
    "housing = train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_num = housing.drop(\"ocean_proximity\",axis=1)\n",
    "housing_labels = train_set[\"median_house_value\"].copy()\n",
    "\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "param_grid = [\n",
    "        {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "        {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "    ]\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "print('Best Parameters: ', grid_search.best_params_)\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49977.69072845478\n"
     ]
    }
   ],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2\n",
    "print(final_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
